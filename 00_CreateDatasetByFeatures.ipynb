{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\r\n",
    "from nltk.corpus import twitter_samples\r\n",
    "from pandas.core.frame import DataFrame\r\n",
    "from Packages.My_NLP import get_most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json') # Load tokenized words from 5000 positive tweets\r\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json') # Load tokenized words from 5000 negative tweets\r\n",
    "\r\n",
    "#Create 10 most frequent wrods from each positive and negative tweets\r\n",
    "most_repetitive_words = get_most_frequent_words(positive_tweet_tokens, negative_tweet_tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')  # Load 5000 raw positive tweet from library as a reference\r\n",
    "dataset_positive = DataFrame(positive_tweets,columns=['Tweet']) # Create a Dataframe by just tweet's text\r\n",
    "dataset_positive[\"Sentimental\"] = \"Positive\" # add positive ( as a sentiment ) to the dataframe's next column\r\n",
    "\r\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json') # Load 5000 raw negative tweet from library as a reference\r\n",
    "dataset_negative = DataFrame(negative_tweets,columns=['Tweet']) # Create a Dataframe by just tweet's text\r\n",
    "dataset_negative[\"Sentimental\"] = \"Negative\" # add negative ( as a sentiment ) to the dataframe's next column\r\n",
    "\r\n",
    "#Create complete dataframe from both negative abd positive tweets + sentiment column\r\n",
    "final_dataset = dataset_positive.append(dataset_negative,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in most_repetitive_words :\r\n",
    "    final_dataset[value[0]]= 0 # add Zero (0) to all cells in final dataset ( dataset initialized by zero value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz = TweetTokenizer()\r\n",
    "for index, row in final_dataset.iterrows(): # read all final dataset ( row by row )\r\n",
    "    for col in most_repetitive_words: # for each keyword \r\n",
    "        TempTokenize = tkz.tokenize(row['Tweet']) # tokenize text of tweet ( in each row )\r\n",
    "        final_dataset.at[index,col[0]] = TempTokenize.count(col[0]) # calculate frequency of keyword in tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained dataset with classifier ( Positive/Negative ) and all kewords frequency ( Features )\r\n",
    "final_dataset = final_dataset.sample(frac=1).reset_index(drop=True)\r\n",
    "sample_dataset = final_dataset.iloc[:20, :]\r\n",
    "trained_dataset = final_dataset.iloc[20:, :]\r\n",
    "sample_dataset.to_csv(\"Exports/Sample_Tweet.csv\",index=False) \r\n",
    "trained_dataset.to_csv(\"Exports/TwitterSentimentDataset.csv\",index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}